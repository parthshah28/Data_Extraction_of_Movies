{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Class of WebCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Web_crawler_IMDB:\n",
    "\n",
    "    def __init__(self):\n",
    "        print('Web Crawler Class Created !!!','\\n==============================')\n",
    "    \n",
    "    #Writing a function that retrives links of next (given number x) pages (Includes given link) \n",
    "    def url_get(self,links,x):\n",
    "        self.url=[links]\n",
    "        self.x=x\n",
    "        for i in range(self.x):\n",
    "            url_req_data = requests.get(self.url[-1])\n",
    "            url_movie_soup = BeautifulSoup(url_req_data.content, 'html.parser')\n",
    "            url_body = url_movie_soup.find_all('div', {'class': 'desc'})[-1]\n",
    "            url_page='https://www.imdb.com'+url_body.find('a',{'class':'lister-page-next next-page'}).get('href')+'&ref_=adv_prv'\n",
    "            self.url.append(url_page)\n",
    "            print('Fetching URLs...')\n",
    "        #URL=self.url\n",
    "        \n",
    "        print('------------------------------',\"\\nURLs Extracted !!!\",'\\n------------------------------')\n",
    "        return self.url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extract_data(self,link):\n",
    "        self.url=link\n",
    "        #A Function that extracts data like Title, Time Period, Rating, Genre, Duration, Votes, Directors, Stars, Description etc.\n",
    "        #These Movie data is extracted from IMDB site\n",
    "        req_data = requests.get(self.url)     #Requesting data of given link\n",
    "        review_soup = BeautifulSoup(req_data.content, 'html.parser')     #Extracting html content of a page\n",
    "        body = review_soup.find_all('div', {'class': 'lister-item mode-advanced'})\n",
    "\n",
    "        #Fetching and storing data in MongoDB\n",
    "        try:\n",
    "            dbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  # opening a connection to MongoDB\n",
    "            db = dbConn['IMDB']\n",
    "            table = db['IMDB_data']\n",
    "            filename = 'IMDB_data'+\".csv\"\n",
    "            fw = open(filename, \"a\")\n",
    "\n",
    "            #Looping over every listed Movie on a perticular page\n",
    "            for movie in body:\n",
    "\n",
    "\n",
    "                #Getting the Title\n",
    "                title_class=movie.find('h3',{'class':'lister-item-header'})\n",
    "                title=title_class.find('a').text\n",
    "\n",
    "\n",
    "\n",
    "                #Getting the time-period\n",
    "                try:\n",
    "                    time_period=title_class.find('span',{'class':'lister-item-year text-muted unbold'}).text.strip()\n",
    "                except:\n",
    "                    time_period=\"Not Found\"\n",
    "\n",
    "\n",
    "\n",
    "                #Getting the genre\n",
    "                try:\n",
    "                    genre=movie.find('span',{'class':'genre'}).text.rstrip().replace('\\n','')\n",
    "                except:\n",
    "                    genre=\"Not Found\"\n",
    "\n",
    "\n",
    "\n",
    "                #Getting Duration of movie\n",
    "                try:\n",
    "                    duration=movie.find('span',{'class':'runtime'}).text[:3]\n",
    "                except:\n",
    "                    duration=\"Not Found\"\n",
    "\n",
    "\n",
    "\n",
    "                #Getting Directors and Stars    \n",
    "                people_inv=movie.find('p',{'class':\"\"}).text.strip().replace('\\n','').split('|')\n",
    "                people_inv=[people.strip() for people in people_inv]\n",
    "                people_inv=[people[people.find(':')+1:] for people in people_inv]\n",
    "                if len(people_inv)==2:\n",
    "                    directors=people_inv[0]\n",
    "                    stars=people_inv[1]\n",
    "                else:\n",
    "                    stars=people_inv[0]\n",
    "                    directors=\"Not Found\"\n",
    "\n",
    "\n",
    "\n",
    "                #Getting the Rating\n",
    "                try:\n",
    "\n",
    "                    rating=movie.find('strong').text\n",
    "                except:\n",
    "                    rating=\"Not Found\"\n",
    "\n",
    "\n",
    "\n",
    "                #Getting Votes\n",
    "                try:\n",
    "                    vot=movie.find('p',{'class':'sort-num_votes-visible'})\n",
    "                    votes=vot.find_all('span')[1].text\n",
    "                except:\n",
    "                    votes=\"Not Found\"\n",
    "\n",
    "\n",
    "\n",
    "                #Getting Description\n",
    "                try:\n",
    "                    description=movie.find_all(\"p\", class_=\"text-muted\")[-1].text.lstrip()\n",
    "                except:\n",
    "                    description='Not Found'\n",
    "\n",
    "\n",
    "\n",
    "                #Writing in the file\n",
    "                fw.write(title+\",\"+time_period + \",\" + rating + \",\" + genre.replace(',',':') + ',' + duration.replace(',',':') + \",\" + votes.replace(',',':')+','+directors.replace(\",\",\":\")+','+stars.replace(',',':')+','+description.replace(',',':')+\"\\n\")\n",
    "\n",
    "\n",
    "                #Creating dictionary for storing it in MongoDB\n",
    "                mydict={\"Title\":title, \"Time Period\":time_period, \"Rating\":rating, \"Genre\":genre, \"Duration\":duration, \"Votes\":votes, \"Directors\":directors, \"Stars\":stars, \"Description\":description}    \n",
    "\n",
    "\n",
    "                #Inserting into Dataset in MongoDB\n",
    "                table.insert_one(mydict)\n",
    "                \n",
    "\n",
    "            #Closing the file\n",
    "            fw.close()\n",
    "            \n",
    "\n",
    "        #For checking is anything wrong with try block\n",
    "        except:\n",
    "            print('Something is wrong')\n",
    "            \n",
    "    #Function for extracting data of any no of links        \n",
    "    def extract_pages(self,link):\n",
    "        self.URL=link\n",
    "        for i in range(len(self.URL)):\n",
    "            self.extract_data(self.URL[i])\n",
    "            print(\"Extracted Data of Page no.:\",i)\n",
    "        print('------------------------------','\\nExtraction Completed !!!','\\n------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a File\n",
    "filename = 'IMDB_data'+\".csv\"\n",
    "fw = open(filename, \"w\")\n",
    "headers = \"Title, Time Period, Rating, Genre, Duration, Votes, Directors, Stars, Description \\n\"\n",
    "fw.write(headers)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Crawler Class Created !!! \n",
      "==============================\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "Fetching URLs...\n",
      "------------------------------ \n",
      "URLs Extracted !!! \n",
      "------------------------------\n",
      "Extracted Data of Page no.: 0\n",
      "Extracted Data of Page no.: 1\n",
      "Extracted Data of Page no.: 2\n",
      "Extracted Data of Page no.: 3\n",
      "Extracted Data of Page no.: 4\n",
      "Extracted Data of Page no.: 5\n",
      "Extracted Data of Page no.: 6\n",
      "Extracted Data of Page no.: 7\n",
      "Extracted Data of Page no.: 8\n",
      "Extracted Data of Page no.: 9\n",
      "Extracted Data of Page no.: 10\n",
      "Extracted Data of Page no.: 11\n",
      "Extracted Data of Page no.: 12\n",
      "Extracted Data of Page no.: 13\n",
      "Extracted Data of Page no.: 14\n",
      "Extracted Data of Page no.: 15\n",
      "Extracted Data of Page no.: 16\n",
      "Extracted Data of Page no.: 17\n",
      "Extracted Data of Page no.: 18\n",
      "Extracted Data of Page no.: 19\n",
      "Extracted Data of Page no.: 20\n",
      "Extracted Data of Page no.: 21\n",
      "Extracted Data of Page no.: 22\n",
      "Extracted Data of Page no.: 23\n",
      "Extracted Data of Page no.: 24\n",
      "Extracted Data of Page no.: 25\n",
      "Extracted Data of Page no.: 26\n",
      "Extracted Data of Page no.: 27\n",
      "Extracted Data of Page no.: 28\n",
      "Extracted Data of Page no.: 29\n",
      "Extracted Data of Page no.: 30\n",
      "Extracted Data of Page no.: 31\n",
      "Extracted Data of Page no.: 32\n",
      "Extracted Data of Page no.: 33\n",
      "Extracted Data of Page no.: 34\n",
      "Extracted Data of Page no.: 35\n",
      "Extracted Data of Page no.: 36\n",
      "Extracted Data of Page no.: 37\n",
      "Extracted Data of Page no.: 38\n",
      "Extracted Data of Page no.: 39\n",
      "Extracted Data of Page no.: 40\n",
      "Extracted Data of Page no.: 41\n",
      "Extracted Data of Page no.: 42\n",
      "Extracted Data of Page no.: 43\n",
      "Extracted Data of Page no.: 44\n",
      "Extracted Data of Page no.: 45\n",
      "Extracted Data of Page no.: 46\n",
      "Extracted Data of Page no.: 47\n",
      "Extracted Data of Page no.: 48\n",
      "Extracted Data of Page no.: 49\n",
      "Extracted Data of Page no.: 50\n",
      "Extracted Data of Page no.: 51\n",
      "Extracted Data of Page no.: 52\n",
      "Extracted Data of Page no.: 53\n",
      "Extracted Data of Page no.: 54\n",
      "Extracted Data of Page no.: 55\n",
      "Extracted Data of Page no.: 56\n",
      "Extracted Data of Page no.: 57\n",
      "Extracted Data of Page no.: 58\n",
      "Extracted Data of Page no.: 59\n",
      "Extracted Data of Page no.: 60\n",
      "Extracted Data of Page no.: 61\n",
      "Extracted Data of Page no.: 62\n",
      "Extracted Data of Page no.: 63\n",
      "Extracted Data of Page no.: 64\n",
      "Extracted Data of Page no.: 65\n",
      "Extracted Data of Page no.: 66\n",
      "Extracted Data of Page no.: 67\n",
      "Extracted Data of Page no.: 68\n",
      "Extracted Data of Page no.: 69\n",
      "Extracted Data of Page no.: 70\n",
      "Extracted Data of Page no.: 71\n",
      "Extracted Data of Page no.: 72\n",
      "Extracted Data of Page no.: 73\n",
      "Extracted Data of Page no.: 74\n",
      "Extracted Data of Page no.: 75\n",
      "Extracted Data of Page no.: 76\n",
      "Extracted Data of Page no.: 77\n",
      "Extracted Data of Page no.: 78\n",
      "Extracted Data of Page no.: 79\n",
      "Extracted Data of Page no.: 80\n",
      "Extracted Data of Page no.: 81\n",
      "Extracted Data of Page no.: 82\n",
      "Extracted Data of Page no.: 83\n",
      "Extracted Data of Page no.: 84\n",
      "Extracted Data of Page no.: 85\n",
      "Extracted Data of Page no.: 86\n",
      "Extracted Data of Page no.: 87\n",
      "Extracted Data of Page no.: 88\n",
      "Extracted Data of Page no.: 89\n",
      "Extracted Data of Page no.: 90\n",
      "Extracted Data of Page no.: 91\n",
      "Extracted Data of Page no.: 92\n",
      "Extracted Data of Page no.: 93\n",
      "Extracted Data of Page no.: 94\n",
      "Extracted Data of Page no.: 95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data of Page no.: 96\n",
      "Extracted Data of Page no.: 97\n",
      "Extracted Data of Page no.: 98\n",
      "Extracted Data of Page no.: 99\n",
      "Extracted Data of Page no.: 100\n",
      "Extracted Data of Page no.: 101\n",
      "Extracted Data of Page no.: 102\n",
      "Extracted Data of Page no.: 103\n",
      "Extracted Data of Page no.: 104\n",
      "Extracted Data of Page no.: 105\n",
      "Extracted Data of Page no.: 106\n",
      "Extracted Data of Page no.: 107\n",
      "Extracted Data of Page no.: 108\n",
      "Extracted Data of Page no.: 109\n",
      "Extracted Data of Page no.: 110\n",
      "Extracted Data of Page no.: 111\n",
      "Extracted Data of Page no.: 112\n",
      "Extracted Data of Page no.: 113\n",
      "Extracted Data of Page no.: 114\n",
      "Extracted Data of Page no.: 115\n",
      "Extracted Data of Page no.: 116\n",
      "Extracted Data of Page no.: 117\n",
      "Extracted Data of Page no.: 118\n",
      "Extracted Data of Page no.: 119\n",
      "Extracted Data of Page no.: 120\n",
      "Extracted Data of Page no.: 121\n",
      "Extracted Data of Page no.: 122\n",
      "Extracted Data of Page no.: 123\n",
      "Extracted Data of Page no.: 124\n",
      "Extracted Data of Page no.: 125\n",
      "Extracted Data of Page no.: 126\n",
      "Extracted Data of Page no.: 127\n",
      "Extracted Data of Page no.: 128\n",
      "Extracted Data of Page no.: 129\n",
      "Extracted Data of Page no.: 130\n",
      "Extracted Data of Page no.: 131\n",
      "Extracted Data of Page no.: 132\n",
      "Extracted Data of Page no.: 133\n",
      "Extracted Data of Page no.: 134\n",
      "Extracted Data of Page no.: 135\n",
      "Extracted Data of Page no.: 136\n",
      "Extracted Data of Page no.: 137\n",
      "Extracted Data of Page no.: 138\n",
      "Extracted Data of Page no.: 139\n",
      "Extracted Data of Page no.: 140\n",
      "Extracted Data of Page no.: 141\n",
      "Extracted Data of Page no.: 142\n",
      "Extracted Data of Page no.: 143\n",
      "Extracted Data of Page no.: 144\n",
      "Extracted Data of Page no.: 145\n",
      "Extracted Data of Page no.: 146\n",
      "Extracted Data of Page no.: 147\n",
      "Extracted Data of Page no.: 148\n",
      "Extracted Data of Page no.: 149\n",
      "Extracted Data of Page no.: 150\n",
      "Extracted Data of Page no.: 151\n",
      "Extracted Data of Page no.: 152\n",
      "Something is wrong\n",
      "Extracted Data of Page no.: 153\n",
      "Extracted Data of Page no.: 154\n",
      "Extracted Data of Page no.: 155\n",
      "Extracted Data of Page no.: 156\n",
      "Extracted Data of Page no.: 157\n",
      "Extracted Data of Page no.: 158\n",
      "Extracted Data of Page no.: 159\n",
      "Extracted Data of Page no.: 160\n",
      "Extracted Data of Page no.: 161\n",
      "Extracted Data of Page no.: 162\n",
      "Extracted Data of Page no.: 163\n",
      "Extracted Data of Page no.: 164\n",
      "Extracted Data of Page no.: 165\n",
      "Extracted Data of Page no.: 166\n",
      "Extracted Data of Page no.: 167\n",
      "Extracted Data of Page no.: 168\n",
      "Extracted Data of Page no.: 169\n",
      "Extracted Data of Page no.: 170\n",
      "Extracted Data of Page no.: 171\n",
      "Extracted Data of Page no.: 172\n",
      "Extracted Data of Page no.: 173\n",
      "Extracted Data of Page no.: 174\n",
      "Extracted Data of Page no.: 175\n",
      "Extracted Data of Page no.: 176\n",
      "Extracted Data of Page no.: 177\n",
      "Extracted Data of Page no.: 178\n",
      "Extracted Data of Page no.: 179\n",
      "Extracted Data of Page no.: 180\n",
      "Extracted Data of Page no.: 181\n",
      "Extracted Data of Page no.: 182\n",
      "Extracted Data of Page no.: 183\n",
      "Extracted Data of Page no.: 184\n",
      "Extracted Data of Page no.: 185\n",
      "Extracted Data of Page no.: 186\n",
      "Extracted Data of Page no.: 187\n",
      "Extracted Data of Page no.: 188\n",
      "Extracted Data of Page no.: 189\n",
      "Extracted Data of Page no.: 190\n",
      "Extracted Data of Page no.: 191\n",
      "Extracted Data of Page no.: 192\n",
      "Extracted Data of Page no.: 193\n",
      "Extracted Data of Page no.: 194\n",
      "Extracted Data of Page no.: 195\n",
      "Extracted Data of Page no.: 196\n",
      "Extracted Data of Page no.: 197\n",
      "Extracted Data of Page no.: 198\n",
      "Extracted Data of Page no.: 199\n",
      "Extracted Data of Page no.: 200\n",
      "Extracted Data of Page no.: 201\n",
      "Extracted Data of Page no.: 202\n",
      "Extracted Data of Page no.: 203\n",
      "Extracted Data of Page no.: 204\n",
      "Extracted Data of Page no.: 205\n",
      "Extracted Data of Page no.: 206\n",
      "Extracted Data of Page no.: 207\n",
      "Extracted Data of Page no.: 208\n",
      "Extracted Data of Page no.: 209\n",
      "Extracted Data of Page no.: 210\n",
      "Extracted Data of Page no.: 211\n",
      "Extracted Data of Page no.: 212\n",
      "Extracted Data of Page no.: 213\n",
      "Extracted Data of Page no.: 214\n",
      "Extracted Data of Page no.: 215\n",
      "Extracted Data of Page no.: 216\n",
      "Extracted Data of Page no.: 217\n",
      "Extracted Data of Page no.: 218\n",
      "Extracted Data of Page no.: 219\n",
      "Extracted Data of Page no.: 220\n",
      "Extracted Data of Page no.: 221\n",
      "Extracted Data of Page no.: 222\n",
      "Extracted Data of Page no.: 223\n",
      "Extracted Data of Page no.: 224\n",
      "Extracted Data of Page no.: 225\n",
      "Extracted Data of Page no.: 226\n",
      "Extracted Data of Page no.: 227\n",
      "Extracted Data of Page no.: 228\n",
      "Extracted Data of Page no.: 229\n",
      "Extracted Data of Page no.: 230\n",
      "Extracted Data of Page no.: 231\n",
      "Extracted Data of Page no.: 232\n",
      "Extracted Data of Page no.: 233\n",
      "Extracted Data of Page no.: 234\n",
      "Extracted Data of Page no.: 235\n",
      "Extracted Data of Page no.: 236\n",
      "Extracted Data of Page no.: 237\n",
      "Extracted Data of Page no.: 238\n",
      "Extracted Data of Page no.: 239\n",
      "Extracted Data of Page no.: 240\n",
      "Extracted Data of Page no.: 241\n",
      "Extracted Data of Page no.: 242\n",
      "Extracted Data of Page no.: 243\n",
      "Extracted Data of Page no.: 244\n",
      "Extracted Data of Page no.: 245\n",
      "Extracted Data of Page no.: 246\n",
      "Extracted Data of Page no.: 247\n",
      "Extracted Data of Page no.: 248\n",
      "Extracted Data of Page no.: 249\n",
      "Extracted Data of Page no.: 250\n",
      "Extracted Data of Page no.: 251\n",
      "Extracted Data of Page no.: 252\n",
      "Extracted Data of Page no.: 253\n",
      "Extracted Data of Page no.: 254\n",
      "Extracted Data of Page no.: 255\n",
      "Extracted Data of Page no.: 256\n",
      "Extracted Data of Page no.: 257\n",
      "Extracted Data of Page no.: 258\n",
      "Extracted Data of Page no.: 259\n",
      "Extracted Data of Page no.: 260\n",
      "Extracted Data of Page no.: 261\n",
      "Extracted Data of Page no.: 262\n",
      "Extracted Data of Page no.: 263\n",
      "Extracted Data of Page no.: 264\n",
      "Extracted Data of Page no.: 265\n",
      "Extracted Data of Page no.: 266\n",
      "Extracted Data of Page no.: 267\n",
      "Extracted Data of Page no.: 268\n",
      "Extracted Data of Page no.: 269\n",
      "Extracted Data of Page no.: 270\n",
      "Extracted Data of Page no.: 271\n",
      "Extracted Data of Page no.: 272\n",
      "Extracted Data of Page no.: 273\n",
      "Extracted Data of Page no.: 274\n",
      "Extracted Data of Page no.: 275\n",
      "Extracted Data of Page no.: 276\n",
      "Extracted Data of Page no.: 277\n",
      "Extracted Data of Page no.: 278\n",
      "Extracted Data of Page no.: 279\n",
      "Extracted Data of Page no.: 280\n",
      "Extracted Data of Page no.: 281\n",
      "Something is wrong\n",
      "Extracted Data of Page no.: 282\n",
      "Extracted Data of Page no.: 283\n",
      "Extracted Data of Page no.: 284\n",
      "Extracted Data of Page no.: 285\n",
      "Extracted Data of Page no.: 286\n",
      "Extracted Data of Page no.: 287\n",
      "Extracted Data of Page no.: 288\n",
      "Extracted Data of Page no.: 289\n",
      "Extracted Data of Page no.: 290\n",
      "Extracted Data of Page no.: 291\n",
      "Extracted Data of Page no.: 292\n",
      "Extracted Data of Page no.: 293\n",
      "Extracted Data of Page no.: 294\n",
      "Extracted Data of Page no.: 295\n",
      "Extracted Data of Page no.: 296\n",
      "Extracted Data of Page no.: 297\n",
      "Extracted Data of Page no.: 298\n",
      "Extracted Data of Page no.: 299\n",
      "Extracted Data of Page no.: 300\n",
      "------------------------------ \n",
      "Extraction Completed !!! \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Giving a link \n",
    "link='https://www.imdb.com/search/title/?title_type=feature,tv_series&count=200&ref_=adv_prv'\n",
    "no_of_next_pages=300\n",
    "\n",
    "#Creating a object from Class Web_crawler_IMDB\n",
    "abc=Web_crawler_IMDB()\n",
    "\n",
    "#Fetching URLs for next pages\n",
    "URL=abc.url_get(link,no_of_next_pages)\n",
    "\n",
    "#Extracting data for given URLS\n",
    "abc.extract_pages(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data from MongoDB and creating a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbConn = pymongo.MongoClient(\"mongodb://localhost:27017/\")  # opening a connection to MongoDB\n",
    "db = dbConn['IMDB']\n",
    "table = db['IMDB_data']\n",
    "\n",
    "#Storing entire data into a list\n",
    "data_mongo=[]\n",
    "for x in table.find():\n",
    "    data_mongo.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataFrame from list\n",
    "mongo_data=pd.DataFrame(data_mongo,columns=['Title','Time Period','Rating','Genre','Duration','Votes','Directors','Stars','Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving DataFrame in a .csv format\n",
    "mongo_data.to_csv('Movies(60000).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Directors</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project Power</td>\n",
       "      <td>(2020)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Action, Crime, Sci-Fi</td>\n",
       "      <td>113</td>\n",
       "      <td>25,225</td>\n",
       "      <td>Henry Joost, Ariel Schulman</td>\n",
       "      <td>Jamie Foxx, Joseph Gordon-Levitt, Dominique Fi...</td>\n",
       "      <td>When a pill that gives its users unpredictable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Tax Collector</td>\n",
       "      <td>(2020)</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>95</td>\n",
       "      <td>3,520</td>\n",
       "      <td>David Ayer</td>\n",
       "      <td>Bobby Soto, Cinthya Carmona, Shia LaBeouf, Jos...</td>\n",
       "      <td>A \"tax collector\" working for a local crime lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perry Mason</td>\n",
       "      <td>(2020– )</td>\n",
       "      <td>7.6</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>60</td>\n",
       "      <td>8,051</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Matthew Rhys, Juliet Rylance, Chris Chalk, She...</td>\n",
       "      <td>In booming 1932 Los Angeles, a down-and-out de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yellowstone</td>\n",
       "      <td>(2018– )</td>\n",
       "      <td>8.5</td>\n",
       "      <td>Drama, Western</td>\n",
       "      <td>60</td>\n",
       "      <td>19,654</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Kevin Costner, Luke Grimes, Kelly Reilly, Wes ...</td>\n",
       "      <td>A ranching family in Montana faces off against...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dark</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>8.8</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>60</td>\n",
       "      <td>241,248</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Louis Hofmann, Karoline Eichhorn, Lisa Vicari,...</td>\n",
       "      <td>A family saga with a supernatural twist, set i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60020</th>\n",
       "      <td>8 Seconds</td>\n",
       "      <td>(2015)</td>\n",
       "      <td>6.5</td>\n",
       "      <td>Biography, Drama, Fantasy</td>\n",
       "      <td>120</td>\n",
       "      <td>1,769</td>\n",
       "      <td>Ömer Faruk Sorak, Birkan Pusa</td>\n",
       "      <td>Esra Inal, Ceylin Adiyaman, Gulay Baltaci, Bat...</td>\n",
       "      <td>ESRA leads two parallel lives. In her worldly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60021</th>\n",
       "      <td>Heart of the West</td>\n",
       "      <td>(1936)</td>\n",
       "      <td>6.9</td>\n",
       "      <td>Western</td>\n",
       "      <td>63</td>\n",
       "      <td>132</td>\n",
       "      <td>Howard Bretherton</td>\n",
       "      <td>William Boyd, James Ellison, George 'Gabby' Ha...</td>\n",
       "      <td>Ranch owner Sally Jordan is engaged in a fence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60022</th>\n",
       "      <td>Matrubhoomi: A Nation Without Women</td>\n",
       "      <td>(2003)</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Drama</td>\n",
       "      <td>99</td>\n",
       "      <td>1,944</td>\n",
       "      <td>Manish Jha</td>\n",
       "      <td>Tulip Joshi, Sudhir Pandey, Sushant Singh, Pan...</td>\n",
       "      <td>A disturbing saga about male chauvinism and mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60023</th>\n",
       "      <td>Talash</td>\n",
       "      <td>(I) (2019)</td>\n",
       "      <td>6.4</td>\n",
       "      <td>Action, Adventure, Comedy</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>32</td>\n",
       "      <td>Zeeshan Khan</td>\n",
       "      <td>Saleem Mairaj, Mustafa Qureshi, Noaman Sami, A...</td>\n",
       "      <td>Add a Plot\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60024</th>\n",
       "      <td>Victim</td>\n",
       "      <td>(I) (2010)</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Horror, Thriller</td>\n",
       "      <td>90</td>\n",
       "      <td>4,156</td>\n",
       "      <td>Matt Eskandari, Michael A. Pierce</td>\n",
       "      <td>Stephen Weigand, Bob Bancroft, Brendan Kelly, ...</td>\n",
       "      <td>A young man finds himself held captive by a my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60025 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title  Time Period Rating  \\\n",
       "0                            Project Power       (2020)    6.0   \n",
       "1                        The Tax Collector       (2020)    4.7   \n",
       "2                              Perry Mason     (2020– )    7.6   \n",
       "3                              Yellowstone     (2018– )    8.5   \n",
       "4                                     Dark  (2017–2020)    8.8   \n",
       "...                                    ...          ...    ...   \n",
       "60020                            8 Seconds       (2015)    6.5   \n",
       "60021                    Heart of the West       (1936)    6.9   \n",
       "60022  Matrubhoomi: A Nation Without Women       (2003)    7.7   \n",
       "60023                               Talash   (I) (2019)    6.4   \n",
       "60024                               Victim   (I) (2010)    6.0   \n",
       "\n",
       "                           Genre   Duration    Votes  \\\n",
       "0          Action, Crime, Sci-Fi        113   25,225   \n",
       "1           Action, Crime, Drama        95     3,520   \n",
       "2                   Crime, Drama        60     8,051   \n",
       "3                 Drama, Western        60    19,654   \n",
       "4          Crime, Drama, Mystery        60   241,248   \n",
       "...                          ...        ...      ...   \n",
       "60020  Biography, Drama, Fantasy        120    1,769   \n",
       "60021                    Western        63       132   \n",
       "60022                      Drama        99     1,944   \n",
       "60023  Action, Adventure, Comedy  Not Found       32   \n",
       "60024           Horror, Thriller        90     4,156   \n",
       "\n",
       "                               Directors  \\\n",
       "0            Henry Joost, Ariel Schulman   \n",
       "1                             David Ayer   \n",
       "2                              Not Found   \n",
       "3                              Not Found   \n",
       "4                              Not Found   \n",
       "...                                  ...   \n",
       "60020      Ömer Faruk Sorak, Birkan Pusa   \n",
       "60021                  Howard Bretherton   \n",
       "60022                         Manish Jha   \n",
       "60023                       Zeeshan Khan   \n",
       "60024  Matt Eskandari, Michael A. Pierce   \n",
       "\n",
       "                                                   Stars  \\\n",
       "0      Jamie Foxx, Joseph Gordon-Levitt, Dominique Fi...   \n",
       "1      Bobby Soto, Cinthya Carmona, Shia LaBeouf, Jos...   \n",
       "2      Matthew Rhys, Juliet Rylance, Chris Chalk, She...   \n",
       "3      Kevin Costner, Luke Grimes, Kelly Reilly, Wes ...   \n",
       "4      Louis Hofmann, Karoline Eichhorn, Lisa Vicari,...   \n",
       "...                                                  ...   \n",
       "60020  Esra Inal, Ceylin Adiyaman, Gulay Baltaci, Bat...   \n",
       "60021  William Boyd, James Ellison, George 'Gabby' Ha...   \n",
       "60022  Tulip Joshi, Sudhir Pandey, Sushant Singh, Pan...   \n",
       "60023  Saleem Mairaj, Mustafa Qureshi, Noaman Sami, A...   \n",
       "60024  Stephen Weigand, Bob Bancroft, Brendan Kelly, ...   \n",
       "\n",
       "                                             Description  \n",
       "0      When a pill that gives its users unpredictable...  \n",
       "1      A \"tax collector\" working for a local crime lo...  \n",
       "2      In booming 1932 Los Angeles, a down-and-out de...  \n",
       "3      A ranching family in Montana faces off against...  \n",
       "4      A family saga with a supernatural twist, set i...  \n",
       "...                                                  ...  \n",
       "60020  ESRA leads two parallel lives. In her worldly ...  \n",
       "60021  Ranch owner Sally Jordan is engaged in a fence...  \n",
       "60022  A disturbing saga about male chauvinism and mi...  \n",
       "60023                                       Add a Plot\\n  \n",
       "60024  A young man finds himself held captive by a my...  \n",
       "\n",
       "[60025 rows x 9 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading Data from .csv file\n",
    "data=pd.read_csv('Movies(60000).csv',index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
